---
sidebar_position: 12
title: Chapter 3.2 - Unity Robotics Hub
description: Game-Engine Based Simulation with Unity
---

# Chapter 3.2: Unity Robotics Hub

## Goal
Explore Unity as a simulation platform for humanoid robotics with high-quality rendering.

## Learning Outcomes
After completing this chapter, students will implement humanoid robot simulation in Unity with sensor integration.

## Introduction

Unity Robotics Hub represents a significant advancement in robotics simulation, leveraging Unity's powerful game engine capabilities for high-fidelity, photorealistic simulation. Unlike traditional physics simulators, Unity excels in visual rendering quality, making it ideal for computer vision tasks, perception system training, and human-robot interaction studies. The Unity Robotics Hub provides the tools and frameworks necessary to integrate Unity with ROS 2, enabling complex humanoid robotics simulation with unprecedented visual fidelity.

## Unity Robotics Hub Overview

### Key Components

The Unity Robotics Hub consists of several key components that facilitate robotics simulation:

1. **Unity Robot Framework**: Provides tools for creating and controlling robots in Unity
2. **ROS-TCP-Connector**: Enables communication between Unity and ROS 2
3. **Synthetic Data Tools**: Generate training data for perception systems
4. **Perception Tools**: Simulate various sensors including cameras, LIDAR, and depth sensors
5. **Robotics Library**: Pre-built components for common robotics tasks

### Advantages of Unity for Robotics

- **High-Quality Rendering**: Photorealistic graphics for computer vision training
- **Flexible Environments**: Create diverse and complex scenarios
- **Realistic Lighting**: Simulate various lighting conditions
- **Asset Store**: Access to thousands of 3D models and environments
- **Cross-Platform**: Deploy to multiple platforms including VR/AR

## Setting Up Unity Robotics Hub

### Prerequisites

Before starting with Unity Robotics Hub, ensure you have:

- Unity Hub and Unity 2021.3 LTS or later
- Unity Robot Framework package
- ROS-TCP-Connector
- Python with ROS 2 environment

### Installation Process

1. **Install Unity Hub**: Download from Unity's official website
2. **Install Unity**: Install version 2021.3 LTS or later
3. **Create New Project**: Create a 3D project for robotics simulation
4. **Install Packages**: Add Unity Robotics packages through Package Manager

### Unity Package Installation

In Unity, navigate to Window > Package Manager and install:

- **ROS TCP Connector**: Enables communication with ROS 2
- **Robot Framework**: Provides robotics-specific components
- **Perception Tools**: For sensor simulation
- **Synthetic Data**: For generating training data

## Creating Humanoid Robot Models in Unity

### Importing Robot Models

Unity supports various 3D model formats (FBX, OBJ, DAE). For humanoid robots, you can:

1. **Convert URDF to Unity**: Use tools like the URDF Importer
2. **Import CAD Models**: Import models directly from CAD software
3. **Create Procedurally**: Build robots using Unity's tools

### Robot Setup in Unity

```csharp
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;

public class HumanoidRobot : MonoBehaviour
{
    // Robot components
    public Transform[] joints;
    public Transform[] links;

    // ROS communication
    private ROSConnection ros;

    // Joint control
    private float[] jointPositions;
    private float[] jointVelocities;

    void Start()
    {
        // Initialize ROS connection
        ros = ROSConnection.instance;

        // Subscribe to joint commands
        ros.Subscribe<sensor_msgs.JointState>("/joint_commands", JointCommandCallback);

        // Initialize joint arrays
        jointPositions = new float[joints.Length];
        jointVelocities = new float[joints.Length];
    }

    void JointCommandCallback(JointState jointState)
    {
        // Process joint commands from ROS
        for (int i = 0; i < jointState.name.Count; i++)
        {
            string jointName = jointState.name[i];
            int jointIndex = FindJointIndex(jointName);

            if (jointIndex >= 0)
            {
                jointPositions[jointIndex] = (float)jointState.position[i];
                jointVelocities[jointIndex] = (float)jointState.velocity[i];
            }
        }
    }

    void Update()
    {
        // Update joint positions in Unity
        for (int i = 0; i < joints.Length; i++)
        {
            // Apply joint position (simplified)
            joints[i].localRotation = Quaternion.Euler(0, jointPositions[i] * Mathf.Rad2Deg, 0);
        }
    }

    int FindJointIndex(string jointName)
    {
        // Find the index of a joint by name
        for (int i = 0; i < joints.Length; i++)
        {
            if (joints[i].name == jointName)
                return i;
        }
        return -1;
    }
}
```

### Joint Configuration

For humanoid robots, proper joint configuration is crucial:

```csharp
using UnityEngine;

[System.Serializable]
public class JointConfig
{
    public string jointName;
    public Joint joint;
    public float minAngle;
    public float maxAngle;
    public float maxVelocity;
    public float maxEffort;
}

public class HumanoidJointController : MonoBehaviour
{
    public JointConfig[] jointConfigs;

    public void SetJointPosition(string jointName, float position)
    {
        JointConfig config = System.Array.Find(jointConfigs, j => j.jointName == jointName);
        if (config != null)
        {
            // Clamp position to joint limits
            position = Mathf.Clamp(position, config.minAngle, config.maxAngle);

            // Apply position to joint
            ConfigurableJoint configurableJoint = config.joint as ConfigurableJoint;
            if (configurableJoint != null)
            {
                // Calculate target rotation
                Quaternion targetRotation = Quaternion.Euler(0, position * Mathf.Rad2Deg, 0);
                configurableJoint.targetRotation = targetRotation;
            }
        }
    }
}
```

## Sensor Integration in Unity

### Camera Sensors

Unity's camera system can simulate various camera types:

```csharp
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;

public class UnityCameraSensor : MonoBehaviour
{
    public Camera unityCamera;
    public string topicName = "/camera/image_raw";
    public int imageWidth = 640;
    public int imageHeight = 480;
    public float publishRate = 30.0f;

    private RenderTexture renderTexture;
    private Texture2D texture2D;
    private ROSConnection ros;
    private float lastPublishTime;

    void Start()
    {
        // Initialize ROS connection
        ros = ROSConnection.instance;

        // Create render texture
        renderTexture = new RenderTexture(imageWidth, imageHeight, 24);
        unityCamera.targetTexture = renderTexture;

        // Create texture for conversion
        texture2D = new Texture2D(imageWidth, imageHeight, TextureFormat.RGB24, false);

        lastPublishTime = Time.time;
    }

    void Update()
    {
        // Check if it's time to publish
        if (Time.time - lastPublishTime >= 1.0f / publishRate)
        {
            PublishCameraImage();
            lastPublishTime = Time.time;
        }
    }

    void PublishCameraImage()
    {
        // Copy render texture to regular texture
        RenderTexture.active = renderTexture;
        texture2D.ReadPixels(new Rect(0, 0, imageWidth, imageHeight), 0, 0);
        texture2D.Apply();

        // Convert to ROS message (simplified)
        byte[] imageData = texture2D.EncodeToPNG();

        // Publish to ROS (implementation depends on specific ROS-TCP-Connector usage)
        // ros.Publish(topicName, imageMsg);
    }

    void OnDestroy()
    {
        if (renderTexture != null)
            renderTexture.Release();
    }
}
```

### LIDAR Simulation

Unity can simulate LIDAR sensors using raycasting:

```csharp
using UnityEngine;
using System.Collections.Generic;

public class UnityLidarSensor : MonoBehaviour
{
    [Header("LIDAR Configuration")]
    public int horizontalRays = 360;
    public int verticalRays = 1;
    public float minRange = 0.1f;
    public float maxRange = 10.0f;
    public float angleMin = -Mathf.PI;
    public float angleMax = Mathf.PI;

    [Header("Performance")]
    public float updateRate = 10.0f;

    private float lastUpdateTime;
    private List<float> ranges;

    void Start()
    {
        ranges = new List<float>(horizontalRays * verticalRays);
        lastUpdateTime = Time.time;
    }

    void Update()
    {
        if (Time.time - lastUpdateTime >= 1.0f / updateRate)
        {
            SimulateLidarScan();
            lastUpdateTime = Time.time;
        }
    }

    void SimulateLidarScan()
    {
        ranges.Clear();

        for (int v = 0; v < verticalRays; v++)
        {
            float verticalAngle = Mathf.Lerp(-0.1f, 0.1f, (float)v / (verticalRays - 1));

            for (int h = 0; h < horizontalRays; h++)
            {
                float horizontalAngle = Mathf.Lerp(angleMin, angleMax, (float)h / (horizontalRays - 1));

                // Calculate ray direction
                Vector3 direction = Quaternion.Euler(verticalAngle * Mathf.Rad2Deg,
                                                   horizontalAngle * Mathf.Rad2Deg, 0) * transform.forward;

                // Raycast
                RaycastHit hit;
                if (Physics.Raycast(transform.position, direction, out hit, maxRange))
                {
                    ranges.Add(hit.distance);
                }
                else
                {
                    ranges.Add(maxRange);
                }
            }
        }
    }

    public float[] GetRanges()
    {
        return ranges.ToArray();
    }
}
```

### IMU Simulation

Simulate IMU data using Unity's physics system:

```csharp
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Sensor;

public class UnityImuSensor : MonoBehaviour
{
    public string topicName = "/imu/data";
    public float noiseLevel = 0.01f;
    public float publishRate = 100.0f;

    private Rigidbody robotBody;
    private ROSConnection ros;
    private float lastPublishTime;

    void Start()
    {
        ros = ROSConnection.instance;
        robotBody = GetComponent<Rigidbody>();
        lastPublishTime = Time.time;
    }

    void Update()
    {
        if (Time.time - lastPublishTime >= 1.0f / publishRate)
        {
            PublishImuData();
            lastPublishTime = Time.time;
        }
    }

    void PublishImuData()
    {
        // Create IMU message (simplified structure)
        Imu imuMsg = new Imu();

        // Orientation from robot's rotation
        Quaternion robotRotation = transform.rotation;
        imuMsg.orientation.x = robotRotation.x;
        imuMsg.orientation.y = robotRotation.y;
        imuMsg.orientation.z = robotRotation.z;
        imuMsg.orientation.w = robotRotation.w;

        // Angular velocity from physics
        Vector3 angularVelocity = robotBody.angularVelocity;
        imuMsg.angular_velocity.x = angularVelocity.x + Random.Range(-noiseLevel, noiseLevel);
        imuMsg.angular_velocity.y = angularVelocity.y + Random.Range(-noiseLevel, noiseLevel);
        imuMsg.angular_velocity.z = angularVelocity.z + Random.Range(-noiseLevel, noiseLevel);

        // Linear acceleration
        Vector3 linearAcc = robotBody.velocity / Time.fixedDeltaTime;
        imuMsg.linear_acceleration.x = linearAcc.x + Random.Range(-noiseLevel, noiseLevel);
        imuMsg.linear_acceleration.y = linearAcc.y + Random.Range(-noiseLevel, noiseLevel);
        imuMsg.linear_acceleration.z = linearAcc.z + Random.Range(-noiseLevel, noiseLevel);

        // Publish to ROS
        ros.Publish(topicName, imuMsg);
    }
}
```

## ROS Communication in Unity

### Setting Up ROS-TCP-Connector

The ROS-TCP-Connector enables communication between Unity and ROS 2:

```csharp
using Unity.Robotics.ROSTCPConnector;

public class UnityRosBridge : MonoBehaviour
{
    public string rosIPAddress = "127.0.0.1";
    public int rosPort = 10000;

    private ROSConnection ros;

    void Start()
    {
        ros = ROSConnection.instance;

        // Connect to ROS
        ros.Initialize(rosIPAddress, rosPort);

        // Subscribe to topics
        ros.Subscribe<sensor_msgs.JointState>("/joint_states", OnJointStateReceived);
        ros.Subscribe<geometry_msgs.Twist>("/cmd_vel", OnCmdVelReceived);

        // Publish to topics
        ros.RegisterPublisher<sensor_msgs.JointState>("/joint_commands");
    }

    void OnJointStateReceived(JointState jointState)
    {
        // Handle received joint states
        Debug.Log($"Received joint state with {jointState.name.Count} joints");
    }

    void OnCmdVelReceived(geometry_msgs.Twist cmdVel)
    {
        // Handle velocity commands
        Debug.Log($"Received velocity command: {cmdVel.linear.x}, {cmdVel.angular.z}");
    }
}
```

### Custom Message Types

For humanoid-specific messages, you can define custom message types:

```csharp
using Unity.Robotics.ROSTCPConnector.MessageGeneration;

public class HumanoidState : Message
{
    public const string k_RosMessageName = "my_robot_msgs/HumanoidState";
    public override string RosMessageName => k_RosMessageName;

    public string[] joint_names;
    public double[] positions;
    public double[] velocities;
    public double[] efforts;
    public geometry_msgs.Pose center_of_mass;
    public bool in_support;

    public HumanoidState()
    {
        joint_names = new string[0];
        positions = new double[0];
        velocities = new double[0];
        efforts = new double[0];
        center_of_mass = new geometry_msgs.Pose();
        in_support = false;
    }

    public HumanoidState(string[] jointNames, double[] positions, double[] velocities,
                        double[] efforts, geometry_msgs.Pose com, bool inSupport)
    {
        this.joint_names = jointNames;
        this.positions = positions;
        this.velocities = velocities;
        this.efforts = efforts;
        this.center_of_mass = com;
        this.in_support = inSupport;
    }

    public static HumanoidState Deserialize(MessageDeserializer deserializer)
    {
        var outMessage = new HumanoidState();
        deserializer.Read(outMessage.joint_names);
        deserializer.Read(outMessage.positions);
        deserializer.Read(outMessage.velocities);
        deserializer.Read(outMessage.efforts);
        deserializer.Read(outMessage.center_of_mass);
        deserializer.Read(outMessage.in_support);
        return outMessage;
    }

    public override void SerializeTo(MessageSerializer serializer)
    {
        serializer.Write(this.joint_names);
        serializer.Write(this.positions);
        serializer.Write(this.velocities);
        serializer.Write(this.efforts);
        serializer.Write(this.center_of_mass);
        serializer.Write(this.in_support);
    }
}
```

## Environment Creation for Humanoid Robots

### Scene Setup

Creating appropriate environments for humanoid robots involves:

1. **Navigation Spaces**: Ensure areas are navigable for bipedal locomotion
2. **Interaction Objects**: Place objects for manipulation tasks
3. **Lighting Conditions**: Vary lighting to test perception systems
4. **Terrain Types**: Include various surfaces for walking simulation

### Procedural Environment Generation

```csharp
using UnityEngine;
using System.Collections.Generic;

public class HumanoidEnvironmentGenerator : MonoBehaviour
{
    public GameObject[] floorTiles;
    public GameObject[] obstacles;
    public GameObject[] interactionObjects;

    [Header("Environment Parameters")]
    public int gridSize = 10;
    public float cellSize = 2.0f;
    public float obstacleDensity = 0.1f;

    void Start()
    {
        GenerateEnvironment();
    }

    void GenerateEnvironment()
    {
        // Create floor grid
        for (int x = 0; x < gridSize; x++)
        {
            for (int z = 0; z < gridSize; z++)
            {
                Vector3 position = new Vector3(x * cellSize, 0, z * cellSize);

                // Choose appropriate floor tile
                int tileIndex = Random.Range(0, floorTiles.Length);
                Instantiate(floorTiles[tileIndex], position, Quaternion.identity);

                // Randomly place obstacles
                if (Random.value < obstacleDensity)
                {
                    int obstacleIndex = Random.Range(0, obstacles.Length);
                    Vector3 obstaclePos = position + new Vector3(0, 1, 0);
                    Instantiate(obstacles[obstacleIndex], obstaclePos, Quaternion.identity);
                }
            }
        }

        // Place interaction objects
        PlaceInteractionObjects();
    }

    void PlaceInteractionObjects()
    {
        // Place objects for humanoid interaction
        for (int i = 0; i < 5; i++)
        {
            Vector3 randomPos = new Vector3(
                Random.Range(2, gridSize * cellSize - 2),
                1.0f,
                Random.Range(2, gridSize * cellSize - 2)
            );

            int objIndex = Random.Range(0, interactionObjects.Length);
            Instantiate(interactionObjects[objIndex], randomPos, Quaternion.identity);
        }
    }
}
```

### Dynamic Environment Elements

```csharp
using UnityEngine;

public class DynamicEnvironmentElement : MonoBehaviour
{
    [Header("Movement Configuration")]
    public bool isMovable = true;
    public Vector3 movementRange = new Vector3(1, 0, 1);
    public float movementSpeed = 1.0f;

    [Header("Interaction Configuration")]
    public bool canBeGrasped = true;
    public float mass = 1.0f;

    private Vector3 initialPosition;
    private Vector3 targetPosition;
    private Rigidbody rb;

    void Start()
    {
        initialPosition = transform.position;
        rb = GetComponent<Rigidbody>();

        if (rb != null)
        {
            rb.mass = mass;
        }

        SetNewTarget();
    }

    void Update()
    {
        if (isMovable)
        {
            MoveTowardsTarget();
        }
    }

    void MoveTowardsTarget()
    {
        transform.position = Vector3.MoveTowards(
            transform.position,
            targetPosition,
            movementSpeed * Time.deltaTime
        );

        // Check if reached target
        if (Vector3.Distance(transform.position, targetPosition) < 0.1f)
        {
            SetNewTarget();
        }
    }

    void SetNewTarget()
    {
        Vector3 randomOffset = new Vector3(
            Random.Range(-movementRange.x, movementRange.x),
            Random.Range(-movementRange.y, movementRange.y),
            Random.Range(-movementRange.z, movementRange.z)
        );

        targetPosition = initialPosition + randomOffset;
    }
}
```

## Physics and Animation for Humanoids

### Character Controller vs Rigidbody

For humanoid robots, you can use either Unity's CharacterController or Rigidbody:

```csharp
using UnityEngine;

public class HumanoidLocomotion : MonoBehaviour
{
    public float walkSpeed = 2.0f;
    public float runSpeed = 4.0f;
    public float jumpForce = 8.0f;

    private CharacterController controller;
    private Vector3 velocity;
    private bool isGrounded;

    void Start()
    {
        controller = GetComponent<CharacterController>();
    }

    void Update()
    {
        // Check if grounded
        isGrounded = controller.isGrounded;
        if (isGrounded && velocity.y < 0)
        {
            velocity.y = -2f;
        }

        // Movement input
        float moveX = Input.GetAxis("Horizontal");
        float moveZ = Input.GetAxis("Vertical");

        Vector3 move = transform.right * moveX + transform.forward * moveZ;

        // Apply movement
        controller.Move(move * walkSpeed * Time.deltaTime);

        // Apply gravity
        velocity.y += Physics.gravity.y * Time.deltaTime;
        controller.Move(velocity * Time.deltaTime);
    }
}
```

### Animation Integration

For more realistic humanoid movement, integrate with Unity's animation system:

```csharp
using UnityEngine;

public class HumanoidAnimatorController : MonoBehaviour
{
    public Animator animator;
    public float walkSpeedThreshold = 0.1f;
    public float runSpeedThreshold = 2.0f;

    [Header("Animation Parameters")]
    public string speedParameter = "Speed";
    public string directionParameter = "Direction";
    public string jumpParameter = "IsJumping";

    private Rigidbody rb;

    void Start()
    {
        rb = GetComponent<Rigidbody>();
        animator = GetComponent<Animator>();
    }

    void Update()
    {
        if (animator != null && rb != null)
        {
            // Calculate movement parameters
            float speed = rb.velocity.magnitude;
            Vector3 localVelocity = transform.InverseTransformDirection(rb.velocity);

            // Update animation parameters
            animator.SetFloat(speedParameter, speed);
            animator.SetFloat(directionParameter, localVelocity.x);
            animator.SetBool(jumpParameter, !IsGrounded());
        }
    }

    bool IsGrounded()
    {
        // Check if the humanoid is grounded
        return Physics.Raycast(transform.position, Vector3.down, 0.1f);
    }
}
```

## Performance Optimization

### Level of Detail (LOD)

For complex humanoid robots, implement LOD systems:

```csharp
using UnityEngine;

public class HumanoidLODController : MonoBehaviour
{
    public GameObject[] lodGroups;
    public float[] lodDistances;

    private Transform viewer;

    void Start()
    {
        viewer = Camera.main.transform;
    }

    void Update()
    {
        float distance = Vector3.Distance(viewer.position, transform.position);

        for (int i = 0; i < lodDistances.Length; i++)
        {
            if (distance <= lodDistances[i])
            {
                ActivateLOD(i);
                return;
            }
        }

        // Activate lowest detail if too far
        ActivateLOD(lodGroups.Length - 1);
    }

    void ActivateLOD(int lodIndex)
    {
        for (int i = 0; i < lodGroups.Length; i++)
        {
            lodGroups[i].SetActive(i == lodIndex);
        }
    }
}
```

### Sensor Performance

Optimize sensor simulation for real-time performance:

```csharp
using UnityEngine;
using System.Collections;

public class OptimizedSensorController : MonoBehaviour
{
    [Header("Performance Settings")]
    public float simulationRate = 30.0f;  // Lower rate for better performance
    public bool useThreading = false;

    private WaitForSeconds waitTime;
    private Coroutine sensorCoroutine;

    void Start()
    {
        waitTime = new WaitForSeconds(1.0f / simulationRate);
        sensorCoroutine = StartCoroutine(SimulateSensors());
    }

    IEnumerator SimulateSensors()
    {
        while (true)
        {
            // Simulate all sensors
            SimulateCameras();
            SimulateLidar();
            SimulateIMU();

            yield return waitTime;
        }
    }

    void SimulateCameras()
    {
        // Simulate camera sensors at reduced frequency
    }

    void SimulateLidar()
    {
        // Simulate LIDAR at reduced resolution
    }

    void SimulateIMU()
    {
        // Simulate IMU data
    }

    void OnDisable()
    {
        if (sensorCoroutine != null)
        {
            StopCoroutine(sensorCoroutine);
        }
    }
}
```

## Best Practices for Unity Robotics

### Model Optimization

- Use appropriate polygon counts for real-time simulation
- Implement efficient collision meshes
- Use texture atlasing for better performance
- Optimize animation rigs for humanoid robots

### Simulation Quality

- Use appropriate physics settings for humanoid dynamics
- Implement proper sensor noise models
- Validate simulation against real-world data
- Test with various environmental conditions

### Integration Considerations

- Ensure consistent coordinate systems between Unity and ROS
- Implement proper time synchronization
- Handle network latency in ROS communication
- Use appropriate message rates for real-time performance

## Troubleshooting Common Issues

### Coordinate System Mismatch

```csharp
// Unity uses left-handed coordinate system, ROS uses right-handed
Vector3 RosToUnity(Vector3 rosVector)
{
    return new Vector3(rosVector.x, rosVector.z, rosVector.y);
}

Vector3 UnityToRos(Vector3 unityVector)
{
    return new Vector3(unityVector.x, unityVector.z, unityVector.y);
}
```

### Performance Issues

- Reduce sensor update rates if needed
- Use simplified collision meshes
- Implement occlusion culling
- Optimize material and shader complexity

## Summary

Unity Robotics Hub provides a powerful platform for high-quality humanoid robot simulation with photorealistic rendering capabilities. The combination of Unity's advanced graphics engine with ROS integration enables new possibilities for perception system training, human-robot interaction studies, and computer vision applications. Proper implementation of sensors, physics, and communication systems is essential for effective humanoid robot simulation in Unity.

## Exercises

1. Create a humanoid robot model in Unity with proper joint configuration
2. Implement camera and IMU sensors with ROS communication
3. Build an environment suitable for humanoid navigation and interaction
4. Optimize your simulation for real-time performance

## References

[1] Unity Technologies, "Unity Robotics Hub Documentation," Unity Technologies, 2023.

[2] S. James et al., "PyBullet: A Real-time Robot Simulation Framework," Conference on Robot Learning, 2017.

[3] J. Tremblay et al., "Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations," ICRA, 2018.